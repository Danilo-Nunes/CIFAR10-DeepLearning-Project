{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e0e54cf",
   "metadata": {},
   "source": [
    "# CIFAR-10 Efficient Training & Distillation Pipeline\n",
    "\n",
    "Este notebook contém uma pipeline completa e comentada para treinar modelos eficientes no CIFAR-10 com foco em aplicação posterior para sistemas tipo CAPTCHA. O fluxo inclui:\n",
    "\n",
    "- carregamento e pré-processamento eficiente com `tf.data`\n",
    "- data augmentation (random crop, flip, brightness/contrast, cutout, mixup)\n",
    "- transferência de aprendizado com backbone pré-treinado em ImageNet (EfficientNetB0)\n",
    "- definição de um *student* leve (MobileNetV2)\n",
    "- treinamento por distilação (teacher -> student)\n",
    "- dicas/células para pruning e quantização e exportação para TFLite\n",
    "\n",
    "OBS: as células de treinamento estão configuradas para rodar de forma segura em notebooks (ex.: Colab). Ajuste `EPOCHS` e `BATCH_SIZE` conforme seu hardware.\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c86c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "print('Eager execution:', tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3c7b01",
   "metadata": {},
   "source": [
    "## 1) Pipeline de dados (tf.data) com augmentations\n",
    "Funções: preprocessamento, random crop, cutout, mixup e construção dos datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9473949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def random_crop_and_resize(image, target_size=96, crop_pad=8):\n",
    "    image = tf.image.resize_with_crop_or_pad(image, target_size + crop_pad, target_size + crop_pad)\n",
    "    image = tf.image.random_crop(image, size=[target_size, target_size, 3])\n",
    "    return image\n",
    "\n",
    "def preprocess_train(image, label, target_size=96):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.image.resize(image, [96,96])\n",
    "    image = random_crop_and_resize(image, target_size=target_size, crop_pad=8)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, 0.06)\n",
    "    image = tf.image.random_contrast(image, 0.95, 1.05)\n",
    "    return image, label\n",
    "\n",
    "def preprocess_eval(image, label, target_size=96):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.image.resize(image, [target_size, target_size])\n",
    "    return image, label\n",
    "\n",
    "def cutout(image, size=16):\n",
    "    h = tf.shape(image)[0]\n",
    "    w = tf.shape(image)[1]\n",
    "    y = tf.random.uniform([], 0, h, dtype=tf.int32)\n",
    "    x = tf.random.uniform([], 0, w, dtype=tf.int32)\n",
    "    y1 = tf.clip_by_value(y - size//2, 0, h)\n",
    "    y2 = tf.clip_by_value(y + size//2, 0, h)\n",
    "    x1 = tf.clip_by_value(x - size//2, 0, w)\n",
    "    x2 = tf.clip_by_value(x + size//2, 0, w)\n",
    "    img = image\n",
    "    img = tf.concat([img[:y1],\n",
    "                     tf.concat([img[y1:y2, :x1], tf.zeros([y2-y1, x2-x1, 3]), img[y1:y2, x2:]], axis=1),\n",
    "                     img[y2:]], axis=0)\n",
    "    return img\n",
    "\n",
    "def mixup(images, labels, alpha=0.2):\n",
    "    if alpha <= 0:\n",
    "        return images, labels\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = tf.shape(images)[0]\n",
    "    index = tf.random.shuffle(tf.range(batch_size))\n",
    "    mixed_images = lam * images + (1 - lam) * tf.gather(images, index)\n",
    "    mixed_labels = lam * labels + (1 - lam) * tf.gather(labels, index)\n",
    "    return mixed_images, mixed_labels\n",
    "\n",
    "def build_datasets(batch_size=128, target_size=96, use_mixup=True, mixup_alpha=0.2):\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    num_classes = 10\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_ds = train_ds.shuffle(50000).map(lambda x,y: preprocess_train(x,y, target_size), num_parallel_calls=AUTOTUNE)\n",
    "    train_ds = train_ds.map(lambda x,y: (cutout(x, size=16), y), num_parallel_calls=AUTOTUNE)\n",
    "    train_ds = train_ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    val_ds = val_ds.map(lambda x,y: preprocess_eval(x,y, target_size), num_parallel_calls=AUTOTUNE)\n",
    "    val_ds = val_ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "\n",
    "    return train_ds, val_ds, (x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8112f11b",
   "metadata": {},
   "source": [
    "## 2) Modelos: Teacher (EfficientNetB0 pré-treinado) e Student (MobileNetV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6008e54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, applications\n",
    "\n",
    "def build_teacher(input_shape=(96,96,3), num_classes=10):\n",
    "    base = tf.keras.applications.EfficientNetB0(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    x = layers.GlobalAveragePooling2D()(base.output)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "    model = models.Model(inputs=base.input, outputs=outputs, name='teacher_effnetb0')\n",
    "    return model\n",
    "\n",
    "def build_student(input_shape=(96,96,3), num_classes=10, alpha=1.0):\n",
    "    base = tf.keras.applications.MobileNetV2(include_top=False, weights=None, input_shape=input_shape, alpha=alpha)\n",
    "    x = layers.GlobalAveragePooling2D()(base.output)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "    model = models.Model(inputs=base.input, outputs=outputs, name='student_mobilenetv2')\n",
    "    return model\n",
    "\n",
    "# Build and show summaries (weights download happens when first instantiated in some envs)\n",
    "teacher = build_teacher()\n",
    "student = build_student()\n",
    "teacher.summary()\n",
    "student.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ebed7c",
   "metadata": {},
   "source": [
    "## 3) Treinamento por distilação (Distiller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb50778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distiller(tf.keras.Model):\n",
    "    def __init__(self, student, teacher, temperature=4.0, alpha=0.5):\n",
    "        super(Distiller, self).__init__()\n",
    "        self.student = student\n",
    "        self.teacher = teacher\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def compile(self, optimizer, metrics, student_loss_fn, distill_loss_fn):\n",
    "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distill_loss_fn = distill_loss_fn\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        teacher_preds = self.teacher(x, training=False)\n",
    "        with tf.GradientTape() as tape:\n",
    "            student_preds = self.student(x, training=True)\n",
    "            s_loss = self.student_loss_fn(y, student_preds)\n",
    "            t_soft = tf.nn.softmax(teacher_preds / self.temperature, axis=1)\n",
    "            s_soft = tf.nn.softmax(student_preds / self.temperature, axis=1)\n",
    "            d_loss = self.distill_loss_fn(t_soft, s_soft)\n",
    "            loss = self.alpha * s_loss + (1.0 - self.alpha) * (self.temperature**2) * d_loss\n",
    "        gradients = tape.gradient(loss, self.student.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.student.trainable_variables))\n",
    "        self.compiled_metrics.update_state(y, student_preds)\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({'loss': loss})\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y_pred = self.student(x, training=False)\n",
    "        t_loss = self.student_loss_fn(y, y_pred)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({'loss': t_loss})\n",
    "        return results\n",
    "\n",
    "def prepare_distiller(teacher, student, lr=1e-3, temperature=4.0, alpha=0.5):\n",
    "    distiller = Distiller(student, teacher, temperature=temperature, alpha=alpha)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    student_loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "    distill_loss = tf.keras.losses.KLDivergence()\n",
    "    distiller.compile(optimizer=optimizer, metrics=[tf.keras.metrics.CategoricalAccuracy()], \n",
    "                      student_loss_fn=student_loss, distill_loss_fn=distill_loss)\n",
    "    return distiller\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1839420d",
   "metadata": {},
   "source": [
    "## 4) Orquestração do treinamento (exemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07897dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste estes valores conforme seu hardware\n",
    "EPOCHS_TEACHER = 20\n",
    "EPOCHS_STUDENT = 40\n",
    "BATCH_SIZE = 128\n",
    "TARGET_SIZE = 96\n",
    "\n",
    "train_ds, val_ds, (x_test, y_test) = build_datasets(batch_size=BATCH_SIZE, target_size=TARGET_SIZE, use_mixup=False)\n",
    "\n",
    "# Treine ou carregue o teacher\n",
    "teacher = build_teacher(input_shape=(TARGET_SIZE,TARGET_SIZE,3))\n",
    "teacher.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "# teacher.fit(train_ds, epochs=EPOCHS_TEACHER, validation_data=val_ds)\n",
    "\n",
    "# Prepare student and distiller\n",
    "student = build_student(input_shape=(TARGET_SIZE,TARGET_SIZE,3), alpha=0.35)\n",
    "distiller = prepare_distiller(teacher, student, lr=1e-3, temperature=4.0, alpha=0.5)\n",
    "# distiller.fit(train_ds, epochs=EPOCHS_STUDENT, validation_data=val_ds)\n",
    "\n",
    "print('Fluxo preparado. Descomente as chamadas .fit() para treinar no seu ambiente.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da767d9",
   "metadata": {},
   "source": [
    "## 5) Pruning e Quantização (snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a17fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tensorflow_model_optimization as tfmot\n",
    "    print('tfmot available for pruning/QAT')\n",
    "    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "    pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.0,\n",
    "                                                                 final_sparsity=0.5,\n",
    "                                                                 begin_step=200,\n",
    "                                                                 end_step=2000)\n",
    "    }\n",
    "    student_for_prune = prune_low_magnitude(student, **pruning_params)\n",
    "    student_for_prune.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # student_for_prune.fit(...)\n",
    "except Exception as e:\n",
    "    print('Pruning not configured (tensorflow_model_optimization missing?) :', e)\n",
    "\n",
    "\n",
    "# TFLite conversion helper\n",
    "def convert_to_tflite(model, quantize=False, representative_data_gen=None, filename='model.tflite'):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    if quantize:\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        if representative_data_gen is not None:\n",
    "            converter.representative_dataset = representative_data_gen\n",
    "    tflite_model = converter.convert()\n",
    "    open(filename, 'wb').write(tflite_model)\n",
    "    print('Saved TFLite model to', filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78de2be",
   "metadata": {},
   "source": [
    "## 6) Export e avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa95c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'saved_student_model'\n",
    "try:\n",
    "    student.save(save_dir, include_optimizer=False)\n",
    "    print('Saved student to', save_dir)\n",
    "except Exception as e:\n",
    "    print('Failed to save student model:', e)\n",
    "\n",
    "# Example convert (may require representative dataset for quantization)\n",
    "try:\n",
    "    convert_to_tflite(student, quantize=False, filename='student_float.tflite')\n",
    "except Exception as e:\n",
    "    print('TFLite conversion failed:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8881724",
   "metadata": {},
   "source": [
    "### Notas finais\n",
    "- Ajuste `EPOCHS` e `BATCH_SIZE` conforme seu hardware.\n",
    "- Para deploy em dispositivos restritos use quantização e pruning.\n",
    "- Treine com exemplos sintéticos de CAPTCHA para melhorar robustez."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
